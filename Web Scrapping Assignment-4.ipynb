{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b6dd7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (4.10.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from selenium) (0.10.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.1)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: idna in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (3.8.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from webdriver-manager) (4.64.1)\n",
      "Requirement already satisfied: requests in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.28.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from webdriver-manager) (21.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from webdriver-manager) (1.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from packaging->webdriver-manager) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2022.9.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\shyamanthularamakris\\anaconda3\\lib\\site-packages (from tqdm->webdriver-manager) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U selenium\n",
    "!pip3 install webdriver-manager\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "import re\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a190eb99",
   "metadata": {},
   "source": [
    "1. Scrape the details of most viewed videos on YouTube from Wikipedia.\n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\n",
    "You need to find following details:\n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96115ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f691be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8f5fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rank=[]\n",
    "Name=[]\n",
    "Artist=[]\n",
    "Upload_date=[]\n",
    "Views=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04a296cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]//tbody//tr//td[1]')[:30]:\n",
    "        Rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Rank.append('-')\n",
    "        \n",
    "\n",
    "#Scraping data of video names\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]//tbody//tr//td[2]')[:30]:\n",
    "        Name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Name.append('-')\n",
    "\n",
    "#Scraping data of artist name\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]//tbody//tr//td[3]')[:30]:\n",
    "        Artist.append(i.text)\n",
    "except NoSuchElementException:        \n",
    "    Artist.append('-')\n",
    "    \n",
    "#Scraping data of upload dates\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]//tbody//tr//td[5]')[:30]:\n",
    "        Upload_date.append(i.text)\n",
    "except NoSuchElementException:        \n",
    "    Upload_date.append('-')\n",
    "    \n",
    "    \n",
    "#Scraping data of views\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]//tbody//tr//td[4]')[:30]:\n",
    "        Views.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Views.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de589f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "Youtube=pd.DataFrame({})\n",
    "Youtube['Rank'] = Rank\n",
    "Youtube['Name'] = Name\n",
    "Youtube['Artist'] = Artist\n",
    "Youtube['Uploaded Date'] = Upload_date\n",
    "Youtube['Views'] = Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79150d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Uploaded Date</th>\n",
       "      <th>Views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>\"Baby Shark Dance\"[4]</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>June 17, 2016</td>\n",
       "      <td>12.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>\"Despacito\"[7]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>January 12, 2017</td>\n",
       "      <td>8.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>\"Johny Johny Yes Papa\"[14]</td>\n",
       "      <td>LooLoo Kids</td>\n",
       "      <td>October 8, 2016</td>\n",
       "      <td>6.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>\"Bath Song\"[15]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>May 2, 2018</td>\n",
       "      <td>6.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>\"Shape of You\"[16]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>January 30, 2017</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.</td>\n",
       "      <td>\"See You Again\"[19]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>April 6, 2015</td>\n",
       "      <td>5.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.</td>\n",
       "      <td>\"Phonics Song with Two Words\"[24]</td>\n",
       "      <td>ChuChu TV</td>\n",
       "      <td>March 6, 2014</td>\n",
       "      <td>5.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.</td>\n",
       "      <td>\"Wheels on the Bus\"[25]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>May 24, 2018</td>\n",
       "      <td>5.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.</td>\n",
       "      <td>\"Uptown Funk\"[26]</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>November 19, 2014</td>\n",
       "      <td>4.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.</td>\n",
       "      <td>\"Learning Colors – Colorful Eggs on a Farm\"[27]</td>\n",
       "      <td>Miroshka TV</td>\n",
       "      <td>February 27, 2018</td>\n",
       "      <td>4.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.</td>\n",
       "      <td>\"Gangnam Style\"[28]</td>\n",
       "      <td>Psy</td>\n",
       "      <td>July 15, 2012</td>\n",
       "      <td>4.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.</td>\n",
       "      <td>\"Masha and the Bear – Recipe for Disaster\"[33]</td>\n",
       "      <td>Get Movies</td>\n",
       "      <td>January 31, 2012</td>\n",
       "      <td>4.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.</td>\n",
       "      <td>\"Dame Tu Cosita\"[34]</td>\n",
       "      <td>El Chombo</td>\n",
       "      <td>April 5, 2018</td>\n",
       "      <td>4.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.</td>\n",
       "      <td>\"Axel F\"[35]</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>June 16, 2009</td>\n",
       "      <td>3.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.</td>\n",
       "      <td>\"Sugar\"[36]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>January 14, 2015</td>\n",
       "      <td>3.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.</td>\n",
       "      <td>\"Roar\"[37]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>September 5, 2013</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.</td>\n",
       "      <td>\"Counting Stars\"[38]</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>May 31, 2013</td>\n",
       "      <td>3.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.</td>\n",
       "      <td>\"Sorry\"[39]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>October 22, 2015</td>\n",
       "      <td>3.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.</td>\n",
       "      <td>\"Baa Baa Black Sheep\"[40]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>June 25, 2018</td>\n",
       "      <td>3.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.</td>\n",
       "      <td>\"Thinking Out Loud\"[41]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>October 7, 2014</td>\n",
       "      <td>3.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.</td>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[42]</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>June 4, 2010</td>\n",
       "      <td>3.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.</td>\n",
       "      <td>\"Dark Horse\"[43]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>February 20, 2014</td>\n",
       "      <td>3.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.</td>\n",
       "      <td>\"Lakdi Ki Kathi\"[44]</td>\n",
       "      <td>Jingle Toons</td>\n",
       "      <td>June 14, 2018</td>\n",
       "      <td>3.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.</td>\n",
       "      <td>\"Faded\"[45]</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>December 3, 2015</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.</td>\n",
       "      <td>\"Perfect\"[46]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>November 9, 2017</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26.</td>\n",
       "      <td>\"Let Her Go\"[47]</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>July 25, 2012</td>\n",
       "      <td>3.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27.</td>\n",
       "      <td>\"Girls Like You\"[48]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>May 31, 2018</td>\n",
       "      <td>3.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.</td>\n",
       "      <td>\"Humpty the train on a fruits ride\"[49]</td>\n",
       "      <td>Kiddiestv Hindi – Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>January 26, 2018</td>\n",
       "      <td>3.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.</td>\n",
       "      <td>\"Lean On\"[50]</td>\n",
       "      <td>Major Lazer</td>\n",
       "      <td>March 22, 2015</td>\n",
       "      <td>3.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.</td>\n",
       "      <td>\"Bailando\"[51]</td>\n",
       "      <td>Enrique Iglesias</td>\n",
       "      <td>April 11, 2014</td>\n",
       "      <td>3.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                             Name  \\\n",
       "0    1.                            \"Baby Shark Dance\"[4]   \n",
       "1    2.                                   \"Despacito\"[7]   \n",
       "2    3.                       \"Johny Johny Yes Papa\"[14]   \n",
       "3    4.                                  \"Bath Song\"[15]   \n",
       "4    5.                               \"Shape of You\"[16]   \n",
       "5    6.                              \"See You Again\"[19]   \n",
       "6    7.                \"Phonics Song with Two Words\"[24]   \n",
       "7    8.                          \"Wheels on the Bus\"[25]   \n",
       "8    9.                                \"Uptown Funk\"[26]   \n",
       "9   10.  \"Learning Colors – Colorful Eggs on a Farm\"[27]   \n",
       "10  11.                              \"Gangnam Style\"[28]   \n",
       "11  12.   \"Masha and the Bear – Recipe for Disaster\"[33]   \n",
       "12  13.                             \"Dame Tu Cosita\"[34]   \n",
       "13  14.                                     \"Axel F\"[35]   \n",
       "14  15.                                      \"Sugar\"[36]   \n",
       "15  16.                                       \"Roar\"[37]   \n",
       "16  17.                             \"Counting Stars\"[38]   \n",
       "17  18.                                      \"Sorry\"[39]   \n",
       "18  19.                        \"Baa Baa Black Sheep\"[40]   \n",
       "19  20.                          \"Thinking Out Loud\"[41]   \n",
       "20  21.           \"Waka Waka (This Time for Africa)\"[42]   \n",
       "21  22.                                 \"Dark Horse\"[43]   \n",
       "22  23.                             \"Lakdi Ki Kathi\"[44]   \n",
       "23  24.                                      \"Faded\"[45]   \n",
       "24  25.                                    \"Perfect\"[46]   \n",
       "25  26.                                 \"Let Her Go\"[47]   \n",
       "26  27.                             \"Girls Like You\"[48]   \n",
       "27  28.          \"Humpty the train on a fruits ride\"[49]   \n",
       "28  29.                                    \"Lean On\"[50]   \n",
       "29  30.                                   \"Bailando\"[51]   \n",
       "\n",
       "                                           Artist      Uploaded Date  Views  \n",
       "0     Pinkfong Baby Shark - Kids' Songs & Stories      June 17, 2016  12.85  \n",
       "1                                      Luis Fonsi   January 12, 2017   8.16  \n",
       "2                                     LooLoo Kids    October 8, 2016   6.70  \n",
       "3                      Cocomelon – Nursery Rhymes        May 2, 2018   6.20  \n",
       "4                                      Ed Sheeran   January 30, 2017   6.00  \n",
       "5                                     Wiz Khalifa      April 6, 2015   5.89  \n",
       "6                                       ChuChu TV      March 6, 2014   5.30  \n",
       "7                      Cocomelon – Nursery Rhymes       May 24, 2018   5.24  \n",
       "8                                     Mark Ronson  November 19, 2014   4.92  \n",
       "9                                     Miroshka TV  February 27, 2018   4.89  \n",
       "10                                            Psy      July 15, 2012   4.80  \n",
       "11                                     Get Movies   January 31, 2012   4.55  \n",
       "12                                      El Chombo      April 5, 2018   4.35  \n",
       "13                                     Crazy Frog      June 16, 2009   3.91  \n",
       "14                                       Maroon 5   January 14, 2015   3.87  \n",
       "15                                     Katy Perry  September 5, 2013   3.80  \n",
       "16                                    OneRepublic       May 31, 2013   3.79  \n",
       "17                                  Justin Bieber   October 22, 2015   3.66  \n",
       "18                     Cocomelon – Nursery Rhymes      June 25, 2018   3.64  \n",
       "19                                     Ed Sheeran    October 7, 2014   3.60  \n",
       "20                                        Shakira       June 4, 2010   3.59  \n",
       "21                                     Katy Perry  February 20, 2014   3.52  \n",
       "22                                   Jingle Toons      June 14, 2018   3.48  \n",
       "23                                    Alan Walker   December 3, 2015   3.45  \n",
       "24                                     Ed Sheeran   November 9, 2017   3.45  \n",
       "25                                      Passenger      July 25, 2012   3.44  \n",
       "26                                       Maroon 5       May 31, 2018   3.42  \n",
       "27  Kiddiestv Hindi – Nursery Rhymes & Kids Songs   January 26, 2018   3.41  \n",
       "28                                    Major Lazer     March 22, 2015   3.38  \n",
       "29                               Enrique Iglesias     April 11, 2014   3.38  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Youtube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb39865",
   "metadata": {},
   "source": [
    "Q2. Scrape the details teamIndia’sinternationalfixtures from bcci.tv. \n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Match title (I.e. 1stODI)\n",
    "B) Series\n",
    "C) Place\n",
    "D) Date\n",
    "E) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc0a92b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "url = \" https://www.bcci.tv/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07f02198",
   "metadata": {},
   "outputs": [],
   "source": [
    "International = driver.find_element(By.XPATH,'/html/body/nav/div[1]/div[2]/ul[1]/li[2]/a') \n",
    "International.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "799d13ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match Title</th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1st Test</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>Windsor Park,</td>\n",
       "      <td>12 JUL 2023</td>\n",
       "      <td>7:30 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2nd Test</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>Queen's Park Oval,</td>\n",
       "      <td>20 JUL 2023</td>\n",
       "      <td>7:30 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1st ODI</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>Kensington Oval,</td>\n",
       "      <td>27 JUL 2023</td>\n",
       "      <td>7:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2nd ODI</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>Kensington Oval,</td>\n",
       "      <td>29 JUL 2023</td>\n",
       "      <td>7:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3rd ODI</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>Brian Lara Stadium,</td>\n",
       "      <td>1 AUG 2023</td>\n",
       "      <td>7:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1st T20I</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>Brian Lara Stadium,</td>\n",
       "      <td>3 AUG 2023</td>\n",
       "      <td>8:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2nd T20I</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>National Stadium,</td>\n",
       "      <td>6 AUG 2023</td>\n",
       "      <td>8:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3rd T20I</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>National Stadium,</td>\n",
       "      <td>8 AUG 2023</td>\n",
       "      <td>8:00 PM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Match Title                          Series                Place  \\\n",
       "0   1st Test   INDIA TOUR OF WEST INDIES 2023        Windsor Park,   \n",
       "1   2nd Test   INDIA TOUR OF WEST INDIES 2023   Queen's Park Oval,   \n",
       "2    1st ODI   INDIA TOUR OF WEST INDIES 2023     Kensington Oval,   \n",
       "3    2nd ODI   INDIA TOUR OF WEST INDIES 2023     Kensington Oval,   \n",
       "4    3rd ODI   INDIA TOUR OF WEST INDIES 2023  Brian Lara Stadium,   \n",
       "5   1st T20I   INDIA TOUR OF WEST INDIES 2023  Brian Lara Stadium,   \n",
       "6   2nd T20I   INDIA TOUR OF WEST INDIES 2023    National Stadium,   \n",
       "7   3rd T20I   INDIA TOUR OF WEST INDIES 2023    National Stadium,   \n",
       "\n",
       "          Date      Time  \n",
       "0  12 JUL 2023  7:30 PM   \n",
       "1  20 JUL 2023  7:30 PM   \n",
       "2  27 JUL 2023  7:00 PM   \n",
       "3  29 JUL 2023  7:00 PM   \n",
       "4   1 AUG 2023  7:00 PM   \n",
       "5   3 AUG 2023  8:00 PM   \n",
       "6   6 AUG 2023  8:00 PM   \n",
       "7   8 AUG 2023  8:00 PM   "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Name=[]\n",
    "Series=[]\n",
    "Place=[]\n",
    "Date=[]\n",
    "Time=[]\n",
    "\n",
    "#Scrapping Name\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//span[@class=\"matchOrderText ng-binding ng-scope\"]'):\n",
    "        Name.append(i.text.replace('-',\"\"))\n",
    "except NoSuchElementException:\n",
    "    Name.append('-')\n",
    "\n",
    "#Scrapping Series\n",
    "try:\n",
    "    \n",
    "    for i in driver.find_elements(By.XPATH,'//h5[@class=\"match-tournament-name ng-binding\"]'):\n",
    "        Series.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Series.append('-')\n",
    "    \n",
    "#scrapping Place\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//span[@class=\"ng-binding ng-scope\"]'):\n",
    "        Place.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Place.append('-')\n",
    "    \n",
    "#Scrapping Date\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//div[@class=\"match-dates ng-binding\"]'):\n",
    "        Date.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Date.append('-')\n",
    "    \n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//div[@class=\"match-time no-margin ng-binding\"]'):\n",
    "        Time.append(i.text.replace('IST',\"\"))\n",
    "except NoSuchElementException:\n",
    "    Time.append('-')\n",
    "\n",
    "    \n",
    "# Creating Dataframe\n",
    "\n",
    "Int_Fixtures=pd.DataFrame({})\n",
    "\n",
    "Int_Fixtures['Match Title']=Name\n",
    "Int_Fixtures['Series']=Series\n",
    "Int_Fixtures['Place']=Place\n",
    "Int_Fixtures['Date']=Date\n",
    "Int_Fixtures['Time']=Time\n",
    "Int_Fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3e3d8d",
   "metadata": {},
   "source": [
    "Q3. Scrape the details of State-wise GDP ofIndia fromstatisticstime.com. \n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details:\n",
    "A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0161c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "url = \"http://statisticstimes.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "654160cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "economy = driver.find_element(By.XPATH,'//div[@class=\"navbar\"]//div[2]//button').click()\n",
    "\n",
    "urls = driver.find_element(By.XPATH,'//div[@class=\"dropdown-content\"]//a[3]')\n",
    "ineco_page = urls.get_attribute(\"href\")\n",
    "#Going to indian economy page\n",
    "driver.get(ineco_page)  \n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cb865b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rank = []\n",
    "State = []\n",
    "GSDP_1 = []\n",
    "GSDP_2 = []\n",
    "Share = []\n",
    "GDP = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67d4c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "sta_GDP = driver.find_element(By.XPATH,'//ul[@style=\"list-style-type:none;margin-left:20px;\"]/li/a')\n",
    "GDP_url = sta_GDP.get_attribute(\"href\")\n",
    "\n",
    "driver.get(GDP_url)\n",
    "time.sleep(6)\n",
    "\n",
    "\n",
    "#Scraping data of rank\n",
    "for i in driver.find_elements(By.XPATH,'//div[@id=\"table_id_wrapper\"]//tbody//tr//td[1]'):\n",
    "    Rank.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of state name\n",
    "for i in driver.find_elements(By.XPATH,'//div[@id=\"table_id_wrapper\"]//tbody//tr//td[2]'):\n",
    "    State.append(i.text)\n",
    "\n",
    "    \n",
    "# Scraping data of GSDP at current price (18-19)\n",
    "for i in driver.find_elements(By.XPATH,'//div[@id=\"table_id_wrapper\"]//tbody//tr//td[3]'):\n",
    "    GSDP_1.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of GSDP at current price (17-18)\n",
    "for i in driver.find_elements(By.XPATH,'//div[@id=\"table_id_wrapper\"]//tbody//tr//td[4]'):\n",
    "    GSDP_2.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of Share(2017)\n",
    "for i in driver.find_elements(By.XPATH,'//div[@id=\"table_id_wrapper\"]//tbody//tr//td[5]'):\n",
    "    Share.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of GDP($ billion)\n",
    "for i in driver.find_elements(By.XPATH,'//div[@id=\"table_id_wrapper\"]//tbody//tr//td[6]'):\n",
    "    GDP.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f675b9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Indian_State</th>\n",
       "      <th>GSDP(19-20)</th>\n",
       "      <th>GSDP(18-19)</th>\n",
       "      <th>Share</th>\n",
       "      <th>GDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>-</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>399.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>1,845,853</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>8.63%</td>\n",
       "      <td>247.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,687,818</td>\n",
       "      <td>1,584,764</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>240.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>-</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>7.96%</td>\n",
       "      <td>228.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,631,977</td>\n",
       "      <td>1,493,127</td>\n",
       "      <td>7.91%</td>\n",
       "      <td>226.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>1,253,832</td>\n",
       "      <td>1,089,898</td>\n",
       "      <td>5.77%</td>\n",
       "      <td>165.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>1,020,989</td>\n",
       "      <td>942,586</td>\n",
       "      <td>4.99%</td>\n",
       "      <td>143.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>972,782</td>\n",
       "      <td>862,957</td>\n",
       "      <td>4.57%</td>\n",
       "      <td>131.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>969,604</td>\n",
       "      <td>861,031</td>\n",
       "      <td>4.56%</td>\n",
       "      <td>130.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>906,672</td>\n",
       "      <td>809,592</td>\n",
       "      <td>4.29%</td>\n",
       "      <td>122.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>-</td>\n",
       "      <td>781,653</td>\n",
       "      <td>4.14%</td>\n",
       "      <td>118.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>856,112</td>\n",
       "      <td>774,870</td>\n",
       "      <td>4.10%</td>\n",
       "      <td>117.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>831,610</td>\n",
       "      <td>734,163</td>\n",
       "      <td>3.89%</td>\n",
       "      <td>111.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>611,804</td>\n",
       "      <td>530,363</td>\n",
       "      <td>2.81%</td>\n",
       "      <td>80.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>574,760</td>\n",
       "      <td>526,376</td>\n",
       "      <td>2.79%</td>\n",
       "      <td>79.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>521,275</td>\n",
       "      <td>487,805</td>\n",
       "      <td>2.58%</td>\n",
       "      <td>74.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>-</td>\n",
       "      <td>315,881</td>\n",
       "      <td>1.67%</td>\n",
       "      <td>47.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>329,180</td>\n",
       "      <td>304,063</td>\n",
       "      <td>1.61%</td>\n",
       "      <td>46.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>328,598</td>\n",
       "      <td>297,204</td>\n",
       "      <td>1.57%</td>\n",
       "      <td>45.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>-</td>\n",
       "      <td>245,895</td>\n",
       "      <td>1.30%</td>\n",
       "      <td>37.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir</td>\n",
       "      <td>-</td>\n",
       "      <td>155,956</td>\n",
       "      <td>0.83%</td>\n",
       "      <td>23.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>165,472</td>\n",
       "      <td>153,845</td>\n",
       "      <td>0.81%</td>\n",
       "      <td>23.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>80,449</td>\n",
       "      <td>73,170</td>\n",
       "      <td>0.39%</td>\n",
       "      <td>11.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>55,984</td>\n",
       "      <td>49,845</td>\n",
       "      <td>0.26%</td>\n",
       "      <td>7.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>-</td>\n",
       "      <td>42,114</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>6.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>38,253</td>\n",
       "      <td>34,433</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>36,572</td>\n",
       "      <td>33,481</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>32,496</td>\n",
       "      <td>28,723</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>31,790</td>\n",
       "      <td>27,870</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>-</td>\n",
       "      <td>27,283</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>-</td>\n",
       "      <td>24,603</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>3.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>26,503</td>\n",
       "      <td>22,287</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>3.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank               Indian_State GSDP(19-20) GSDP(18-19)   Share      GDP\n",
       "0     1                Maharashtra           -   2,632,792  13.94%  399.921\n",
       "1     2                 Tamil Nadu   1,845,853   1,630,208   8.63%  247.629\n",
       "2     3              Uttar Pradesh   1,687,818   1,584,764   8.39%  240.726\n",
       "3     4                    Gujarat           -   1,502,899   7.96%  228.290\n",
       "4     5                  Karnataka   1,631,977   1,493,127   7.91%  226.806\n",
       "5     6                West Bengal   1,253,832   1,089,898   5.77%  165.556\n",
       "6     7                  Rajasthan   1,020,989     942,586   4.99%  143.179\n",
       "7     8             Andhra Pradesh     972,782     862,957   4.57%  131.083\n",
       "8     9                  Telangana     969,604     861,031   4.56%  130.791\n",
       "9    10             Madhya Pradesh     906,672     809,592   4.29%  122.977\n",
       "10   11                     Kerala           -     781,653   4.14%  118.733\n",
       "11   12                      Delhi     856,112     774,870   4.10%  117.703\n",
       "12   13                    Haryana     831,610     734,163   3.89%  111.519\n",
       "13   14                      Bihar     611,804     530,363   2.81%   80.562\n",
       "14   15                     Punjab     574,760     526,376   2.79%   79.957\n",
       "15   16                     Odisha     521,275     487,805   2.58%   74.098\n",
       "16   17                      Assam           -     315,881   1.67%   47.982\n",
       "17   18               Chhattisgarh     329,180     304,063   1.61%   46.187\n",
       "18   19                  Jharkhand     328,598     297,204   1.57%   45.145\n",
       "19   20                Uttarakhand           -     245,895   1.30%   37.351\n",
       "20   21            Jammu & Kashmir           -     155,956   0.83%   23.690\n",
       "21   22           Himachal Pradesh     165,472     153,845   0.81%   23.369\n",
       "22   23                        Goa      80,449      73,170   0.39%   11.115\n",
       "23   24                    Tripura      55,984      49,845   0.26%    7.571\n",
       "24   25                 Chandigarh           -      42,114   0.22%    6.397\n",
       "25   26                 Puducherry      38,253      34,433   0.18%    5.230\n",
       "26   27                  Meghalaya      36,572      33,481   0.18%    5.086\n",
       "27   28                     Sikkim      32,496      28,723   0.15%    4.363\n",
       "28   29                    Manipur      31,790      27,870   0.15%    4.233\n",
       "29   30                   Nagaland           -      27,283   0.14%    4.144\n",
       "30   31          Arunachal Pradesh           -      24,603   0.13%    3.737\n",
       "31   32                    Mizoram      26,503      22,287   0.12%    3.385\n",
       "32   33  Andaman & Nicobar Islands           -           -       -        -"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=list(zip(Rank,State,GSDP_1,GSDP_2,Share,GDP))\n",
    "df=pd.DataFrame(data,columns=['Rank','Indian_State','GSDP(19-20)','GSDP(18-19)','Share','GDP'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b183065",
   "metadata": {},
   "source": [
    " Q4.Scrape the details of trending repositories on Github.com. \n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6986bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Downloading: 100%|██████████| 6.30M/6.30M [00:00<00:00, 8.30MB/s]\n"
     ]
    }
   ],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "url='https://github.com/'\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3161901",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Getting explore button and clicking on it\n",
    "explore = driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/button\")\n",
    "\n",
    "#Selecting trending option\n",
    "trend_url = driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/div/div[3]/ul/li[2]/a\")\n",
    "urls = trend_url.get_attribute(\"href\")\n",
    "driver.get(urls)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# making empty lists    \n",
    "repo_urls=[]\n",
    "Contributors_count=[]\n",
    "Language_used=[]\n",
    "repo_title=[]\n",
    "repo_desc=[]\n",
    "\n",
    "#scraping repositories urls\n",
    "repos=driver.find_elements(By.XPATH,\"//h2[@class='h3 lh-condensed']/a\")\n",
    "for i in repos:\n",
    "    repo_urls.append(i.get_attribute('href'))\n",
    "time.sleep(2)    \n",
    "    \n",
    "#scraping repositories title\n",
    "repo_title=[]\n",
    "try:\n",
    "    repos=driver.find_elements(By.XPATH,\"//h2[@class='h3 lh-condensed']\")\n",
    "    for i in repos:\n",
    "        repo_title.append(i.text)\n",
    "except NoSuchElementException:#handling no such element exception\n",
    "    repo_title.append('No details available')\n",
    "time.sleep(3)\n",
    "    \n",
    "    \n",
    "for i in repo_urls:\n",
    "    driver.get(i)\n",
    "    l=[]\n",
    "    time.sleep(3)\n",
    "     #scraping repositories discription\n",
    "    try:          \n",
    "        repo_desc.append(driver.find_element(By.XPATH,\"//p[@class='f4 my-3']\").text)\n",
    "    except:\n",
    "        repo_desc.append('-')\n",
    "      #scraping contributors count\n",
    "    try:\n",
    "        count=driver.find_element(By.XPATH,\"//h2[@class='h4 mb-3']/a[contains(text(),'Contributors')]/span\")\n",
    "        Contributors_count.append(count.text)\n",
    "    except NoSuchElementException:#handling no such element exception\n",
    "        Contributors_count.append('No details available')\n",
    "    time.sleep(2)\n",
    "    \n",
    "     #scraping languages used   \n",
    "    languages=driver.find_elements(By.XPATH,\"//li[@class='d-inline']//a//span[1]\")\n",
    "    if languages:    \n",
    "        for i in languages:\n",
    "            l.append(i.text)\n",
    "    else:\n",
    "        l.append('No languages used')\n",
    "    Language_used.append(l)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad43876d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 25 25 25\n"
     ]
    }
   ],
   "source": [
    "print(len(repo_title),len(repo_desc),len(Contributors_count),len(Language_used))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1099805b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Contributors_count</th>\n",
       "      <th>Language_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sveltejs / svelte</td>\n",
       "      <td>Cybernetically enhanced web apps</td>\n",
       "      <td>605</td>\n",
       "      <td>[JavaScript, Svelte]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a16z-infra / ai-getting-started</td>\n",
       "      <td>A Javascript AI getting started stack for week...</td>\n",
       "      <td>5</td>\n",
       "      <td>[TypeScript, JavaScript, PLpgSQL, Dockerfile, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vllm-project / vllm</td>\n",
       "      <td>A high-throughput and memory-efficient inferen...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Python, Cuda]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>figma / plugin-samples</td>\n",
       "      <td>🔌 Sample Figma plugins.</td>\n",
       "      <td>19</td>\n",
       "      <td>[TypeScript, HTML, JavaScript, CSS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google / googletest</td>\n",
       "      <td>GoogleTest - Google Testing and Mocking Framework</td>\n",
       "      <td>399</td>\n",
       "      <td>[C++, Python, CMake, C, Starlark, Shell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OpenDriveLab / UniAD</td>\n",
       "      <td>[CVPR 2023 Best Paper] Planning-oriented Auton...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Python, Shell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>csunny / DB-GPT</td>\n",
       "      <td>Revolutionizing Database Interactions with Pri...</td>\n",
       "      <td>19</td>\n",
       "      <td>[Python, Shell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>s0md3v / sd-webui-roop</td>\n",
       "      <td>roop extension for StableDiffusion web-ui</td>\n",
       "      <td>No details available</td>\n",
       "      <td>[Python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>devfullcycle / imersao13</td>\n",
       "      <td>-</td>\n",
       "      <td>2</td>\n",
       "      <td>[TypeScript, Go, JavaScript]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>codecrafters-io / build-your-own-x</td>\n",
       "      <td>Master programming by recreating your favorite...</td>\n",
       "      <td>104</td>\n",
       "      <td>[No languages used]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Jessecar96 / SteamDesktopAuthenticator</td>\n",
       "      <td>Desktop implementation of Steam's mobile authe...</td>\n",
       "      <td>16</td>\n",
       "      <td>[C#, Batchfile]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ossu / computer-science</td>\n",
       "      <td>🎓 Path to a free self-taught education in Comp...</td>\n",
       "      <td>132</td>\n",
       "      <td>[No languages used]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TodePond / DreamBerd</td>\n",
       "      <td>perfect programming language</td>\n",
       "      <td>21</td>\n",
       "      <td>[No languages used]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SkalskiP / top-cvpr-2023-papers</td>\n",
       "      <td>This repository is a curated collection of the...</td>\n",
       "      <td>2</td>\n",
       "      <td>[Python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>serverless-stack / sst</td>\n",
       "      <td>💥 SST makes it easy to build full-stack server...</td>\n",
       "      <td>199</td>\n",
       "      <td>[TypeScript, JavaScript, CSS, HTML, Python, Sv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>scikit-learn / scikit-learn</td>\n",
       "      <td>scikit-learn: machine learning in Python</td>\n",
       "      <td>2,672</td>\n",
       "      <td>[Python, Cython, C++, C, Shell, Makefile]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>angular / angular</td>\n",
       "      <td>The modern web developer’s platform</td>\n",
       "      <td>1,742</td>\n",
       "      <td>[TypeScript, JavaScript, Starlark, HTML, CSS, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AUTOMATIC1111 / stable-diffusion-webui</td>\n",
       "      <td>Stable Diffusion web UI</td>\n",
       "      <td>417</td>\n",
       "      <td>[Python, JavaScript, HTML, CSS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>eslint / eslint</td>\n",
       "      <td>Find and fix problems in your JavaScript code.</td>\n",
       "      <td>952</td>\n",
       "      <td>[JavaScript]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>loft-sh / devpod</td>\n",
       "      <td>Spin up dev environments in any infra. Dev-env...</td>\n",
       "      <td>21</td>\n",
       "      <td>[Go, TypeScript, Rust, Shell, JavaScript, Dock...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>nodejs / node</td>\n",
       "      <td>Node.js JavaScript runtime ✨🐢🚀✨</td>\n",
       "      <td>3,252</td>\n",
       "      <td>[JavaScript, C++, Python, C, HTML, Shell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>edtechre / pybroker</td>\n",
       "      <td>Algorithmic Trading in Python with Machine Lea...</td>\n",
       "      <td>2</td>\n",
       "      <td>[Python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>veler / DevToys</td>\n",
       "      <td>A Swiss Army knife for developers.</td>\n",
       "      <td>66</td>\n",
       "      <td>[C#, CSS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>geohot / tinygrad</td>\n",
       "      <td>You like pytorch? You like micrograd? You love...</td>\n",
       "      <td>146</td>\n",
       "      <td>[Python, C, C++, Objective-C++, Shell, Assembly]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>terraform-aws-modules / terraform-aws-eks</td>\n",
       "      <td>Terraform module to create an Elastic Kubernet...</td>\n",
       "      <td>320</td>\n",
       "      <td>[HCL, Smarty]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Title  \\\n",
       "0                           sveltejs / svelte   \n",
       "1             a16z-infra / ai-getting-started   \n",
       "2                         vllm-project / vllm   \n",
       "3                      figma / plugin-samples   \n",
       "4                         google / googletest   \n",
       "5                        OpenDriveLab / UniAD   \n",
       "6                             csunny / DB-GPT   \n",
       "7                      s0md3v / sd-webui-roop   \n",
       "8                    devfullcycle / imersao13   \n",
       "9          codecrafters-io / build-your-own-x   \n",
       "10     Jessecar96 / SteamDesktopAuthenticator   \n",
       "11                    ossu / computer-science   \n",
       "12                       TodePond / DreamBerd   \n",
       "13            SkalskiP / top-cvpr-2023-papers   \n",
       "14                     serverless-stack / sst   \n",
       "15                scikit-learn / scikit-learn   \n",
       "16                          angular / angular   \n",
       "17     AUTOMATIC1111 / stable-diffusion-webui   \n",
       "18                            eslint / eslint   \n",
       "19                           loft-sh / devpod   \n",
       "20                              nodejs / node   \n",
       "21                        edtechre / pybroker   \n",
       "22                            veler / DevToys   \n",
       "23                          geohot / tinygrad   \n",
       "24  terraform-aws-modules / terraform-aws-eks   \n",
       "\n",
       "                                          Description    Contributors_count  \\\n",
       "0                    Cybernetically enhanced web apps                   605   \n",
       "1   A Javascript AI getting started stack for week...                     5   \n",
       "2   A high-throughput and memory-efficient inferen...                     4   \n",
       "3                             🔌 Sample Figma plugins.                    19   \n",
       "4   GoogleTest - Google Testing and Mocking Framework                   399   \n",
       "5   [CVPR 2023 Best Paper] Planning-oriented Auton...                     5   \n",
       "6   Revolutionizing Database Interactions with Pri...                    19   \n",
       "7           roop extension for StableDiffusion web-ui  No details available   \n",
       "8                                                   -                     2   \n",
       "9   Master programming by recreating your favorite...                   104   \n",
       "10  Desktop implementation of Steam's mobile authe...                    16   \n",
       "11  🎓 Path to a free self-taught education in Comp...                   132   \n",
       "12                       perfect programming language                    21   \n",
       "13  This repository is a curated collection of the...                     2   \n",
       "14  💥 SST makes it easy to build full-stack server...                   199   \n",
       "15           scikit-learn: machine learning in Python                 2,672   \n",
       "16                The modern web developer’s platform                 1,742   \n",
       "17                            Stable Diffusion web UI                   417   \n",
       "18     Find and fix problems in your JavaScript code.                   952   \n",
       "19  Spin up dev environments in any infra. Dev-env...                    21   \n",
       "20                    Node.js JavaScript runtime ✨🐢🚀✨                 3,252   \n",
       "21  Algorithmic Trading in Python with Machine Lea...                     2   \n",
       "22                 A Swiss Army knife for developers.                    66   \n",
       "23  You like pytorch? You like micrograd? You love...                   146   \n",
       "24  Terraform module to create an Elastic Kubernet...                   320   \n",
       "\n",
       "                                        Language_used  \n",
       "0                                [JavaScript, Svelte]  \n",
       "1   [TypeScript, JavaScript, PLpgSQL, Dockerfile, ...  \n",
       "2                                      [Python, Cuda]  \n",
       "3                 [TypeScript, HTML, JavaScript, CSS]  \n",
       "4            [C++, Python, CMake, C, Starlark, Shell]  \n",
       "5                                     [Python, Shell]  \n",
       "6                                     [Python, Shell]  \n",
       "7                                            [Python]  \n",
       "8                        [TypeScript, Go, JavaScript]  \n",
       "9                                 [No languages used]  \n",
       "10                                    [C#, Batchfile]  \n",
       "11                                [No languages used]  \n",
       "12                                [No languages used]  \n",
       "13                                           [Python]  \n",
       "14  [TypeScript, JavaScript, CSS, HTML, Python, Sv...  \n",
       "15          [Python, Cython, C++, C, Shell, Makefile]  \n",
       "16  [TypeScript, JavaScript, Starlark, HTML, CSS, ...  \n",
       "17                    [Python, JavaScript, HTML, CSS]  \n",
       "18                                       [JavaScript]  \n",
       "19  [Go, TypeScript, Rust, Shell, JavaScript, Dock...  \n",
       "20          [JavaScript, C++, Python, C, HTML, Shell]  \n",
       "21                                           [Python]  \n",
       "22                                          [C#, CSS]  \n",
       "23   [Python, C, C++, Objective-C++, Shell, Assembly]  \n",
       "24                                      [HCL, Smarty]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(2)   \n",
    "#creating dataframe\n",
    "df=pd.DataFrame({'Title':repo_title,\n",
    "                'Description':repo_desc,\n",
    "                'Contributors_count':Contributors_count,\n",
    "                'Language_used':Language_used})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e0984",
   "metadata": {},
   "source": [
    "Q5. Scrape the details of top 100 songs on billiboard.com. \n",
    "Url = https:/www.billboard.com/\n",
    "You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88d83958",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "url = \"https://www.billboard.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "230b1253",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH,\"/html/body/div[3]/header/div/div[2]/div/div/div[2]/div[2]/div/div/nav/ul/li[1]/a\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a15eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH,\"//div[@class='lrv-u-flex lrv-u-justify-content-center']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f9a1d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name = []\n",
    "Artist = []\n",
    "Last_week_rank = []\n",
    "Peak_rank = []\n",
    "Weeks_on_board = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11e9b563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Last Night',\n",
       " 'Flowers',\n",
       " 'Fast Car',\n",
       " 'Calm Down',\n",
       " 'All My Life',\n",
       " 'Favorite Song',\n",
       " 'Kill Bill',\n",
       " \"Creepin'\",\n",
       " 'Karma',\n",
       " 'Ella Baila Sola',\n",
       " 'Sure Thing',\n",
       " 'Anti-Hero',\n",
       " 'Die For You',\n",
       " 'Something In The Orange',\n",
       " 'Snooze',\n",
       " 'La Bebe',\n",
       " 'Where She Goes',\n",
       " 'Un x100to',\n",
       " 'Need A Favor',\n",
       " 'Search & Rescue',\n",
       " 'You Proof',\n",
       " \"Thinkin' Bout Me\",\n",
       " 'Chemical',\n",
       " 'Cupid',\n",
       " 'Rock And A Hard Place',\n",
       " 'Eyes Closed',\n",
       " \"Boy's A Liar, Pt. 2\",\n",
       " 'Next Thing You Know',\n",
       " 'Put It On Da Floor Again',\n",
       " 'Thought You Should Know',\n",
       " \"I'm Good (Blue)\",\n",
       " 'Dance The Night',\n",
       " 'Area Codes',\n",
       " \"Dancin' In The Country\",\n",
       " 'One Thing At A Time',\n",
       " 'Memory Lane',\n",
       " 'Bzrp Music Sessions, Vol. 55',\n",
       " 'Tennessee Orange',\n",
       " 'Cruel Summer',\n",
       " 'TQM',\n",
       " 'Stand By Me',\n",
       " 'Religiously',\n",
       " 'Dial Drunk',\n",
       " 'Under The Influence',\n",
       " 'Players',\n",
       " 'Calling',\n",
       " 'Annihilate',\n",
       " 'Take Two',\n",
       " 'Love You Anyway',\n",
       " 'Thank God',\n",
       " 'Am I Dreaming',\n",
       " 'Princess Diana',\n",
       " 'Bye',\n",
       " 'Self Love',\n",
       " 'It Matters To Her',\n",
       " 'Daylight',\n",
       " 'PRC',\n",
       " 'Por Las Noches',\n",
       " 'Mourning',\n",
       " 'What It Is (Block Boy)',\n",
       " 'See You Again',\n",
       " 'TQG',\n",
       " 'El Azul',\n",
       " 'Bury Me In Georgia',\n",
       " 'Waffle House',\n",
       " 'You, Me, & Whiskey',\n",
       " 'Your Heart Or Mine',\n",
       " 'Plebada',\n",
       " 'Cowgirls',\n",
       " 'ICU',\n",
       " \"Baby Don't Hurt Me\",\n",
       " 'Popular',\n",
       " 'Slut Me Out',\n",
       " 'Peaches & Eggplants',\n",
       " 'America Has A Problem',\n",
       " 'Fight The Feeling',\n",
       " \"Ain't That Some\",\n",
       " 'Shake Sumn',\n",
       " 'Chanel',\n",
       " 'People',\n",
       " 'Bread & Butter',\n",
       " 'Trustfall',\n",
       " 'Fragil',\n",
       " 'Hits Different',\n",
       " 'Jaded',\n",
       " 'Pound Town 2',\n",
       " 'Low Down',\n",
       " 'Ocean Spray',\n",
       " 'Truck Bed',\n",
       " 'Moonlight',\n",
       " 'Trance',\n",
       " 'All The Way Live',\n",
       " 'The Hillbillies',\n",
       " 'El Gordo Trae El Mando',\n",
       " 'Too Many Nights',\n",
       " 'Save Me',\n",
       " 'Yandel 150',\n",
       " 'Beso',\n",
       " 'I Wrote The Book',\n",
       " 'Hummingbird']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scraping song names\n",
    "try:\n",
    "    names=driver.find_elements(By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"]//h3')\n",
    "    for i in names:\n",
    "        Name.append(i.text)\n",
    "except NoSuchElementException:       \n",
    "    Name.append('No details available')\n",
    "time.sleep(2)    \n",
    "Name[::4][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b8acc5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Morgan Wallen',\n",
       " 'Miley Cyrus',\n",
       " 'Luke Combs',\n",
       " 'Rema & Selena Gomez',\n",
       " 'Lil Durk Featuring J. Cole',\n",
       " 'Toosii',\n",
       " 'SZA',\n",
       " 'Metro Boomin, The Weeknd & 21 Savage',\n",
       " 'Taylor Swift Featuring Ice Spice',\n",
       " 'Eslabon Armado X Peso Pluma',\n",
       " 'Miguel',\n",
       " 'Taylor Swift',\n",
       " 'The Weeknd & Ariana Grande',\n",
       " 'Zach Bryan',\n",
       " 'SZA',\n",
       " 'Yng Lvcas x Peso Pluma',\n",
       " 'Bad Bunny',\n",
       " 'Grupo Frontera X Bad Bunny',\n",
       " 'Jelly Roll',\n",
       " 'Drake',\n",
       " 'Morgan Wallen',\n",
       " 'Morgan Wallen',\n",
       " 'Post Malone',\n",
       " 'Fifty Fifty',\n",
       " 'Bailey Zimmerman',\n",
       " 'Ed Sheeran',\n",
       " 'PinkPantheress & Ice Spice',\n",
       " 'Jordan Davis',\n",
       " 'Latto Featuring Cardi B',\n",
       " 'Morgan Wallen',\n",
       " 'David Guetta & Bebe Rexha',\n",
       " 'Dua Lipa',\n",
       " 'Kali',\n",
       " 'Tyler Hubbard',\n",
       " 'Morgan Wallen',\n",
       " 'Old Dominion',\n",
       " 'Bizarrap & Peso Pluma',\n",
       " 'Megan Moroney',\n",
       " 'Taylor Swift',\n",
       " 'Fuerza Regida',\n",
       " 'Lil Durk Featuring Morgan Wallen',\n",
       " 'Bailey Zimmerman',\n",
       " 'Noah Kahan',\n",
       " 'Chris Brown',\n",
       " 'Coi Leray',\n",
       " 'Metro Boomin, Swae Lee & NAV Featuring A Boogie Wit da Hoodie',\n",
       " 'Metro Boomin, Swae Lee, Lil Wayne & Offset',\n",
       " 'BTS',\n",
       " 'Luke Combs',\n",
       " 'Kane Brown With Katelyn Brown',\n",
       " 'Metro Boomin, A$AP Rocky & Roisee',\n",
       " 'Ice Spice & Nicki Minaj',\n",
       " 'Peso Pluma',\n",
       " 'Metro Boomin & Coi Leray',\n",
       " 'Scotty McCreery',\n",
       " 'David Kushner',\n",
       " 'Peso Pluma X Natanael Cano',\n",
       " 'Peso Pluma',\n",
       " 'Post Malone',\n",
       " 'Doechii Featuring Kodak Black',\n",
       " 'Tyler, The Creator Featuring Kali Uchis',\n",
       " 'Karol G x Shakira',\n",
       " 'Junior H x Peso Pluma',\n",
       " 'Kane Brown',\n",
       " 'Jonas Brothers',\n",
       " 'Justin Moore & Priscilla Block',\n",
       " 'Jon Pardi',\n",
       " 'El Alfa x Peso Pluma',\n",
       " 'Morgan Wallen Featuring ERNEST',\n",
       " 'Coco Jones',\n",
       " 'David Guetta, Anne-Marie & Coi Leray',\n",
       " 'The Weeknd, Playboi Carti & Madonna',\n",
       " 'NLE Choppa',\n",
       " 'Young Nudy Featuring 21 Savage',\n",
       " 'Beyonce Featuring Kendrick Lamar',\n",
       " 'Rod Wave',\n",
       " 'Morgan Wallen',\n",
       " 'DaBaby',\n",
       " 'Becky G & Peso Pluma',\n",
       " 'Libianca',\n",
       " 'Gunna',\n",
       " 'P!nk',\n",
       " 'Yahritza y Su Esencia x Grupo Frontera',\n",
       " 'Taylor Swift',\n",
       " 'Miley Cyrus',\n",
       " 'Sexyy Red & Tay Keith & Nicki Minaj',\n",
       " 'Lil Baby',\n",
       " 'Moneybagg Yo',\n",
       " 'HARDY',\n",
       " 'Kali Uchis',\n",
       " 'Metro Boomin, Travis Scott & Young Thug',\n",
       " 'Metro Boomin, Future & Lil Uzi Vert',\n",
       " 'Baby Keem & Kendrick Lamar',\n",
       " 'Chino Pacas',\n",
       " 'Metro Boomin Featuring Don Toliver & Future',\n",
       " 'Jelly Roll With Lainey Wilson',\n",
       " 'Yandel & Feid',\n",
       " 'Rosalia & Rauw Alejandro',\n",
       " 'Morgan Wallen',\n",
       " 'Metro Boomin & James Blake']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scraping artist names\n",
    "try:\n",
    "    artist=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]//span')\n",
    "    for i in artist:\n",
    "        Artist.append(i.text)\n",
    "except NoSuchElementException:     \n",
    "    Artist.append('-')\n",
    "time.sleep(2)    \n",
    "Artist[::7][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc159d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '2',\n",
       " '4',\n",
       " '3',\n",
       " '5',\n",
       " '7',\n",
       " '6',\n",
       " '9',\n",
       " '10',\n",
       " '8',\n",
       " '12',\n",
       " '11',\n",
       " '14',\n",
       " '15',\n",
       " '19',\n",
       " '23',\n",
       " '16',\n",
       " '17',\n",
       " '20',\n",
       " '18',\n",
       " '22',\n",
       " '24',\n",
       " '25',\n",
       " '21',\n",
       " '27',\n",
       " '33',\n",
       " '26',\n",
       " '38',\n",
       " '13',\n",
       " '32',\n",
       " '29',\n",
       " '35',\n",
       " '37',\n",
       " '36',\n",
       " '30',\n",
       " '42',\n",
       " '31',\n",
       " '45',\n",
       " '47',\n",
       " '34',\n",
       " '28',\n",
       " '50',\n",
       " '-',\n",
       " '46',\n",
       " '39',\n",
       " '41',\n",
       " '44',\n",
       " '-',\n",
       " '52',\n",
       " '40',\n",
       " '51',\n",
       " '53',\n",
       " '54',\n",
       " '62',\n",
       " '49',\n",
       " '56',\n",
       " '55',\n",
       " '57',\n",
       " '58',\n",
       " '73',\n",
       " '60',\n",
       " '63',\n",
       " '64',\n",
       " '77',\n",
       " '74',\n",
       " '72',\n",
       " '80',\n",
       " '-',\n",
       " '76',\n",
       " '75',\n",
       " '82',\n",
       " '43',\n",
       " '67',\n",
       " '92',\n",
       " '66',\n",
       " '81',\n",
       " '83',\n",
       " '85',\n",
       " '78',\n",
       " '91',\n",
       " '48',\n",
       " '100',\n",
       " '88',\n",
       " '65',\n",
       " '98',\n",
       " '68',\n",
       " '89',\n",
       " '69',\n",
       " '-',\n",
       " '97',\n",
       " '96',\n",
       " '61',\n",
       " '-',\n",
       " '93',\n",
       " '-',\n",
       " '86',\n",
       " '-',\n",
       " '94',\n",
       " '-',\n",
       " '90']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scraping last week rank of the song\n",
    "try:\n",
    "    last_rank=driver.find_elements(By.XPATH,'//li[@class=\"o-chart-results-list__item // a-chart-color u-width-72 u-width-55@mobile-max u-width-55@tablet-only lrv-u-flex lrv-u-flex-shrink-0 lrv-u-align-items-center lrv-u-justify-content-center lrv-u-border-b-1 u-border-b-0@mobile-max lrv-u-border-color-grey-light u-background-color-white-064@mobile-max u-hidden@mobile-max\"]')\n",
    "    for i in last_rank:\n",
    "        Last_week_rank.append(i.text)\n",
    "except NoSuchElementException:      \n",
    "    Last_week_rank.append('-')\n",
    "    time.sleep(2)    \n",
    "Last_week_rank[::2][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f9d6e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '1',\n",
       " '3',\n",
       " '3',\n",
       " '2',\n",
       " '5',\n",
       " '1',\n",
       " '3',\n",
       " '2',\n",
       " '4',\n",
       " '11',\n",
       " '1',\n",
       " '1',\n",
       " '10',\n",
       " '15',\n",
       " '11',\n",
       " '8',\n",
       " '5',\n",
       " '19',\n",
       " '2',\n",
       " '5',\n",
       " '9',\n",
       " '13',\n",
       " '17',\n",
       " '10',\n",
       " '19',\n",
       " '3',\n",
       " '28',\n",
       " '13',\n",
       " '7',\n",
       " '4',\n",
       " '32',\n",
       " '33',\n",
       " '23',\n",
       " '10',\n",
       " '36',\n",
       " '31',\n",
       " '30',\n",
       " '29',\n",
       " '34',\n",
       " '22',\n",
       " '40',\n",
       " '43',\n",
       " '12',\n",
       " '9',\n",
       " '41',\n",
       " '44',\n",
       " '48',\n",
       " '15',\n",
       " '13',\n",
       " '51',\n",
       " '4',\n",
       " '53',\n",
       " '54',\n",
       " '49',\n",
       " '47',\n",
       " '33',\n",
       " '28',\n",
       " '36',\n",
       " '60',\n",
       " '44',\n",
       " '7',\n",
       " '55',\n",
       " '64',\n",
       " '57',\n",
       " '66',\n",
       " '67',\n",
       " '68',\n",
       " '40',\n",
       " '63',\n",
       " '71',\n",
       " '43',\n",
       " '28',\n",
       " '74',\n",
       " '38',\n",
       " '16',\n",
       " '11',\n",
       " '69',\n",
       " '55',\n",
       " '80',\n",
       " '48',\n",
       " '82',\n",
       " '69',\n",
       " '27',\n",
       " '56',\n",
       " '66',\n",
       " '50',\n",
       " '69',\n",
       " '89',\n",
       " '80',\n",
       " '42',\n",
       " '61',\n",
       " '93',\n",
       " '58',\n",
       " '22',\n",
       " '86',\n",
       " '71',\n",
       " '52',\n",
       " '18',\n",
       " '90']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scraping song's peak rank\n",
    "try:\n",
    "    peak=driver.find_elements(By.XPATH,'//li[@class=\"o-chart-results-list__item // a-chart-bg-color a-chart-color u-width-72 u-width-55@mobile-max u-width-55@tablet-only lrv-u-flex lrv-u-flex-shrink-0 lrv-u-align-items-center lrv-u-justify-content-center lrv-u-background-color-grey-lightest lrv-u-border-b-1 u-border-b-0@mobile-max lrv-u-border-color-grey-light u-hidden@mobile-max\"]')\n",
    "    for i in peak:\n",
    "        Peak_rank.append(i.text)\n",
    "except NoSuchElementException:       \n",
    "    Peak_rank.append('-')\n",
    "time.sleep(2)    \n",
    "Peak_rank[1::2][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01d9900e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20',\n",
       " '22',\n",
       " '12',\n",
       " '41',\n",
       " '5',\n",
       " '17',\n",
       " '27',\n",
       " '28',\n",
       " '14',\n",
       " '13',\n",
       " '46',\n",
       " '34',\n",
       " '46',\n",
       " '60',\n",
       " '27',\n",
       " '13',\n",
       " '4',\n",
       " '9',\n",
       " '11',\n",
       " '10',\n",
       " '57',\n",
       " '15',\n",
       " '9',\n",
       " '13',\n",
       " '53',\n",
       " '12',\n",
       " '19',\n",
       " '21',\n",
       " '2',\n",
       " '44',\n",
       " '42',\n",
       " '3',\n",
       " '6',\n",
       " '17',\n",
       " '28',\n",
       " '11',\n",
       " '2',\n",
       " '27',\n",
       " '6',\n",
       " '4',\n",
       " '3',\n",
       " '6',\n",
       " '1',\n",
       " '40',\n",
       " '24',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '18',\n",
       " '40',\n",
       " '2',\n",
       " '9',\n",
       " '3',\n",
       " '2',\n",
       " '9',\n",
       " '9',\n",
       " '18',\n",
       " '14',\n",
       " '4',\n",
       " '6',\n",
       " '9',\n",
       " '16',\n",
       " '10',\n",
       " '5',\n",
       " '7',\n",
       " '6',\n",
       " '5',\n",
       " '1',\n",
       " '15',\n",
       " '13',\n",
       " '4',\n",
       " '2',\n",
       " '13',\n",
       " '2',\n",
       " '5',\n",
       " '11',\n",
       " '15',\n",
       " '6',\n",
       " '9',\n",
       " '7',\n",
       " '2',\n",
       " '3',\n",
       " '8',\n",
       " '3',\n",
       " '5',\n",
       " '3',\n",
       " '13',\n",
       " '2',\n",
       " '1',\n",
       " '10',\n",
       " '19',\n",
       " '2',\n",
       " '1',\n",
       " '13',\n",
       " '13',\n",
       " '2',\n",
       " '17',\n",
       " '12',\n",
       " '19',\n",
       " '2']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scraping number of weeks on board\n",
    "try:\n",
    "    weeks=driver.find_elements(By.XPATH,'//li[@class=\"o-chart-results-list__item // a-chart-color u-width-72 u-width-55@mobile-max u-width-55@tablet-only lrv-u-flex lrv-u-flex-shrink-0 lrv-u-align-items-center lrv-u-justify-content-center lrv-u-border-b-1 u-border-b-0@mobile-max lrv-u-border-color-grey-light u-background-color-white-064@mobile-max u-hidden@mobile-max\"]')\n",
    "    for i in weeks:\n",
    "        Weeks_on_board.append(i.text)\n",
    "except NoSuchElementException:       #handling no such element exception\n",
    "    Weeks_on_board.append('-')\n",
    "time.sleep(2)    \n",
    "Weeks_on_board[1::2][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "205db0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song_Name</th>\n",
       "      <th>Artist_Name</th>\n",
       "      <th>Last_week_rank</th>\n",
       "      <th>Peak</th>\n",
       "      <th>Weeks_on_chart</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last Night</td>\n",
       "      <td>Morgan Wallen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Flowers</td>\n",
       "      <td>Miley Cyrus</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fast Car</td>\n",
       "      <td>Luke Combs</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Calm Down</td>\n",
       "      <td>Rema &amp; Selena Gomez</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All My Life</td>\n",
       "      <td>Lil Durk Featuring J. Cole</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Save Me</td>\n",
       "      <td>Jelly Roll With Lainey Wilson</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Yandel 150</td>\n",
       "      <td>Yandel &amp; Feid</td>\n",
       "      <td>-</td>\n",
       "      <td>71</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Beso</td>\n",
       "      <td>Rosalia &amp; Rauw Alejandro</td>\n",
       "      <td>94</td>\n",
       "      <td>52</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>I Wrote The Book</td>\n",
       "      <td>Morgan Wallen</td>\n",
       "      <td>-</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Hummingbird</td>\n",
       "      <td>Metro Boomin &amp; James Blake</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Song_Name                    Artist_Name Last_week_rank Peak  \\\n",
       "0         Last Night                  Morgan Wallen              1    1   \n",
       "1            Flowers                    Miley Cyrus              2    1   \n",
       "2           Fast Car                     Luke Combs              4    3   \n",
       "3          Calm Down            Rema & Selena Gomez              3    3   \n",
       "4        All My Life     Lil Durk Featuring J. Cole              5    2   \n",
       "..               ...                            ...            ...  ...   \n",
       "95           Save Me  Jelly Roll With Lainey Wilson             86   86   \n",
       "96        Yandel 150                  Yandel & Feid              -   71   \n",
       "97              Beso       Rosalia & Rauw Alejandro             94   52   \n",
       "98  I Wrote The Book                  Morgan Wallen              -   18   \n",
       "99       Hummingbird     Metro Boomin & James Blake             90   90   \n",
       "\n",
       "   Weeks_on_chart  \n",
       "0              20  \n",
       "1              22  \n",
       "2              12  \n",
       "3              41  \n",
       "4               5  \n",
       "..            ...  \n",
       "95              2  \n",
       "96             17  \n",
       "97             12  \n",
       "98             19  \n",
       "99              2  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Song_Name':Name[::4][:100],\n",
    "                'Artist_Name':Artist[::7][:100],\n",
    "                'Last_week_rank':Last_week_rank[::2][:100],\n",
    "                'Peak':Peak_rank[1::2][:100],\n",
    "                'Weeks_on_chart':Weeks_on_board[1::2][:100]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d0b9d9",
   "metadata": {},
   "source": [
    "Q6. Scrape the details of Highest sellingnovels.\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey\u0002compare\n",
    "You have to find the following details:\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e32d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1afebc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "Book_name = []\n",
    "Author = []\n",
    "Volumes_sold = []\n",
    "Publisher = []\n",
    "Genre = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e79e3d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in driver.find_elements(By.XPATH,\"//tbody//tr/td[2]\"):\n",
    "    Book_name.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of author names\n",
    "for i in driver.find_elements(By.XPATH,\"//tbody/tr/td[3]\"):\n",
    "    try:\n",
    "        if i.text == '0' : raise NoSuchElementException\n",
    "        Author.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        Author.append('-')\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "#Scraping data of volumes sold\n",
    "for i in driver.find_elements(By.XPATH,\"//tbody/tr/td[4]\"):\n",
    "    Volumes_sold.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of publisher names\n",
    "for i in driver.find_elements(By.XPATH,\"//tbody/tr/td[5]\"):\n",
    "    Publisher.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of genre\n",
    "for i in driver.find_elements(By.XPATH,\"//tbody/tr/td[6]\"):\n",
    "    Genre.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "605465a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Name</th>\n",
       "      <th>Author</th>\n",
       "      <th>Volume sold</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Book Name            Author  \\\n",
       "0                                   Da Vinci Code,The        Brown, Dan   \n",
       "1                Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2            Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3           Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4                                Fifty Shades of Grey      James, E. L.   \n",
       "..                                                ...               ...   \n",
       "95                                          Ghost,The    Harris, Robert   \n",
       "96                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "   Volume sold        Publisher                        Genre  \n",
       "0    5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1    4,475,152       Bloomsbury           Children's Fiction  \n",
       "2    4,200,654       Bloomsbury           Children's Fiction  \n",
       "3    4,179,479       Bloomsbury           Children's Fiction  \n",
       "4    3,758,936     Random House              Romance & Sagas  \n",
       "..         ...              ...                          ...  \n",
       "95     807,311     Random House   General & Literary Fiction  \n",
       "96     794,201          Penguin        Food & Drink: General  \n",
       "97     792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98     791,507            Orion           Biography: General  \n",
       "99     791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Novel=pd.DataFrame({})\n",
    "Novel['Book Name'] = Book_name\n",
    "Novel['Author'] = Author\n",
    "Novel['Volume sold'] = Volumes_sold\n",
    "Novel['Publisher'] = Publisher\n",
    "Novel['Genre'] = Genre\n",
    "Novel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3259bca9",
   "metadata": {},
   "source": [
    "Q7.Scrape the details most watched tv series of all time from imdb.com. \n",
    "Url = https://www.imdb.com/list/ls095964455/\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a5ef6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "url = \"https://www.imdb.com/list/ls095964455\"\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4975d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name = []\n",
    "Year_span = []\n",
    "Genre = []\n",
    "Run_time = []\n",
    "Ratings = []\n",
    "Votes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17f2976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Names\n",
    "for i in driver.find_elements(By.XPATH,\"//h3[@class='lister-item-header']/a\"):\n",
    "    Name.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of Year span\n",
    "for i in driver.find_elements(By.XPATH,\"//span[@class='lister-item-year text-muted unbold']\"):\n",
    "    Year_span.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of genre\n",
    "for i in driver.find_elements(By.XPATH,\"//span[@class='genre']\"):\n",
    "    Genre.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of Run time\n",
    "for i in driver.find_elements(By.XPATH,\"//span[@class='runtime']\"):\n",
    "    Run_time.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of Ratings\n",
    "for i in driver.find_elements(By.XPATH,\"//div[@class='ipl-rating-star small']//span[2]\"):\n",
    "    Ratings.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of votes\n",
    "for i in driver.find_elements(By.XPATH,\"//div[@class='lister-item-content']//p[4]/span[2]\"):\n",
    "    Votes.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2756a4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year Span</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Run Time</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>(2011–2019)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>57 min</td>\n",
       "      <td>9.2</td>\n",
       "      <td>2,171,895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>(2016–2024)</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1,249,981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>(2010–2022)</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1,031,635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13 Reasons Why</td>\n",
       "      <td>(2017–2020)</td>\n",
       "      <td>Drama, Mystery, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>303,353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 100</td>\n",
       "      <td>(2014–2020)</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>262,532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Reign</td>\n",
       "      <td>(2013–2017)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>42 min</td>\n",
       "      <td>7.4</td>\n",
       "      <td>51,919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A Series of Unfortunate Events</td>\n",
       "      <td>(2017–2019)</td>\n",
       "      <td>Adventure, Comedy, Drama</td>\n",
       "      <td>50 min</td>\n",
       "      <td>7.8</td>\n",
       "      <td>63,951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Criminal Minds</td>\n",
       "      <td>(2005– )</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>42 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>208,409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Scream: The TV Series</td>\n",
       "      <td>(2015–2019)</td>\n",
       "      <td>Comedy, Crime, Drama</td>\n",
       "      <td>45 min</td>\n",
       "      <td>7.1</td>\n",
       "      <td>43,369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The Haunting of Hill House</td>\n",
       "      <td>(2018)</td>\n",
       "      <td>Drama, Horror, Mystery</td>\n",
       "      <td>572 min</td>\n",
       "      <td>8.6</td>\n",
       "      <td>259,849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Name    Year Span                     Genre  \\\n",
       "0                  Game of Thrones  (2011–2019)  Action, Adventure, Drama   \n",
       "1                  Stranger Things  (2016–2024)    Drama, Fantasy, Horror   \n",
       "2                 The Walking Dead  (2010–2022)   Drama, Horror, Thriller   \n",
       "3                   13 Reasons Why  (2017–2020)  Drama, Mystery, Thriller   \n",
       "4                          The 100  (2014–2020)    Drama, Mystery, Sci-Fi   \n",
       "..                             ...          ...                       ...   \n",
       "95                           Reign  (2013–2017)                     Drama   \n",
       "96  A Series of Unfortunate Events  (2017–2019)  Adventure, Comedy, Drama   \n",
       "97                  Criminal Minds     (2005– )     Crime, Drama, Mystery   \n",
       "98           Scream: The TV Series  (2015–2019)      Comedy, Crime, Drama   \n",
       "99      The Haunting of Hill House       (2018)    Drama, Horror, Mystery   \n",
       "\n",
       "   Run Time Ratings      Votes  \n",
       "0    57 min     9.2  2,171,895  \n",
       "1    51 min     8.7  1,249,981  \n",
       "2    44 min     8.1  1,031,635  \n",
       "3    60 min     7.5    303,353  \n",
       "4    43 min     7.6    262,532  \n",
       "..      ...     ...        ...  \n",
       "95   42 min     7.4     51,919  \n",
       "96   50 min     7.8     63,951  \n",
       "97   42 min     8.1    208,409  \n",
       "98   45 min     7.1     43,369  \n",
       "99  572 min     8.6    259,849  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMDB=pd.DataFrame({})\n",
    "IMDB['Name'] = Name\n",
    "IMDB['Year Span'] = Year_span\n",
    "IMDB['Genre'] = Genre\n",
    "IMDB['Run Time'] = Run_time\n",
    "IMDB['Ratings'] = Ratings\n",
    "IMDB['Votes'] = Votes\n",
    "IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c67c5d0",
   "metadata": {},
   "source": [
    "Q8. Details of Datasetsfrom UCI machine learning repositories. \n",
    "Url = https://archive.ics.uci.edu/\n",
    "You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute\n",
    "G) Year\n",
    "Note: - from the home page you have to go to the ShowAllDataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1883e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "336cef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding view all dataset button\n",
    "view_dataset = driver.find_element(By.XPATH,\"//a[@class='btn-primary btn']\")    \n",
    "page_url = view_dataset.get_attribute(\"href\")\n",
    "driver.get(page_url)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5fc5e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "\n",
    "    dataset_url = driver.find_elements(By.XPATH,\"//a[@class='link-hover link text-xl font-semibold']\")\n",
    "    urls = []\n",
    "    for i in dataset_url:\n",
    "        urls.append(i.get_attribute(\"href\"))\n",
    "    \n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH,\"//button[2][@class='btn-primary btn-sm btn']\")\n",
    "        next_button.click()\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92f84ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f2b4ce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_name = []\n",
    "Data_type = []\n",
    "Task = []\n",
    "Attribute_type = []\n",
    "No_of_instances = []\n",
    "No_of_attributes = []\n",
    "Year = []\n",
    "\n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "     # scraping  Dataset name\n",
    "    try:\n",
    "        dataset_name = driver.find_element(By.XPATH,\"//h1[@class='text-3xl font-semibold text-primary-content']\")\n",
    "        Dataset_name.append(dataset_name.text)\n",
    "    except NoSuchElementException:\n",
    "        Dataset_name.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # scraping data type\n",
    "    try:\n",
    "        data_type = driver.find_element(By.XPATH,\"//div[1][@class='col-span-4']/p\")\n",
    "        if data_type.text == \"N/A\": raise NoSuchElementException\n",
    "        Data_type.append(data_type.text)\n",
    "    except NoSuchElementException:\n",
    "        Data_type.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # scraping Task\n",
    "    try:\n",
    "        task = driver.find_element(By.XPATH,\"//div[3][@class='col-span-4']/p\")\n",
    "        if task.text == \"N/A\": raise NoSuchElementException\n",
    "        Task.append(task.text)\n",
    "    except NoSuchElementException:\n",
    "        Task.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # scraping Attribute type\n",
    "    try:\n",
    "        attribute_type = driver.find_element(By.XPATH,\"//div[4][@class='col-span-4']/p\")\n",
    "        if attribute_type.text == \"N/A\": raise NoSuchElementException\n",
    "        Attribute_type.append(attribute_type.text)\n",
    "    except NoSuchElementException:\n",
    "        Attribute_type.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "     # scraping No of Instances\n",
    "    try:\n",
    "        instances = driver.find_element(By.XPATH,\"//div[5][@class='col-span-4']/p\")\n",
    "        if instances.text == \"N/A\": raise NoSuchElementException\n",
    "        No_of_instances.append(instances.text)\n",
    "    except NoSuchElementException:\n",
    "        No_of_instances.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # scraping No of Arrtibutes\n",
    "    try:\n",
    "        attribute = driver.find_element(By.XPATH,\"//div[6][@class='col-span-4']/p\")\n",
    "        if attribute.text == \"N/A\": raise NoSuchElementException\n",
    "        No_of_attributes.append(attribute.text)\n",
    "    except NoSuchElementException:\n",
    "        No_of_attributes.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    # scraping Year\n",
    "    try:\n",
    "        year = driver.find_element(By.XPATH,\"//h2[@class='text-primary-content']\")\n",
    "        if year.text == \"N/A\": raise NoSuchElementException\n",
    "        Year.append(year.text[16:])\n",
    "    except NoSuchElementException:\n",
    "        Year.append('-')\n",
    "    time.sleep(3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c5de7c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Name</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Task</th>\n",
       "      <th>Attribute Type</th>\n",
       "      <th>No of Instance</th>\n",
       "      <th>No of Attributes</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>150</td>\n",
       "      <td>4</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heart Disease</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>303</td>\n",
       "      <td>13</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>48842</td>\n",
       "      <td>14</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dry Bean Dataset</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>13611</td>\n",
       "      <td>17</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diabetes</td>\n",
       "      <td>Multivariate, Time-Series</td>\n",
       "      <td>-</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>-</td>\n",
       "      <td>20</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rice (Cammeo and Osmancik)</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>3810</td>\n",
       "      <td>8</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wine</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>178</td>\n",
       "      <td>13</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Car Evaluation</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>1728</td>\n",
       "      <td>6</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Breast Cancer Wisconsin (Diagnostic)</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>569</td>\n",
       "      <td>30</td>\n",
       "      <td>/1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mushroom</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>8124</td>\n",
       "      <td>22</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Data Name                 Data Type   \\\n",
       "0                                  Iris               Multivariate   \n",
       "1                         Heart Disease               Multivariate   \n",
       "2                                 Adult               Multivariate   \n",
       "3                      Dry Bean Dataset               Multivariate   \n",
       "4                              Diabetes  Multivariate, Time-Series   \n",
       "5            Rice (Cammeo and Osmancik)               Multivariate   \n",
       "6                                  Wine               Multivariate   \n",
       "7                        Car Evaluation               Multivariate   \n",
       "8  Breast Cancer Wisconsin (Diagnostic)               Multivariate   \n",
       "9                              Mushroom               Multivariate   \n",
       "\n",
       "            Task              Attribute Type  No of Instance   \\\n",
       "0  Classification                        Real             150   \n",
       "1  Classification  Categorical, Integer, Real             303   \n",
       "2  Classification        Categorical, Integer           48842   \n",
       "3  Classification               Integer, Real           13611   \n",
       "4               -        Categorical, Integer               -   \n",
       "5  Classification                        Real            3810   \n",
       "6  Classification               Integer, Real             178   \n",
       "7  Classification                 Categorical            1728   \n",
       "8  Classification                        Real             569   \n",
       "9  Classification                 Categorical            8124   \n",
       "\n",
       "  No of Attributes   Year   \n",
       "0                 4   1988  \n",
       "1                13   1988  \n",
       "2                14   1996  \n",
       "3                17   2020  \n",
       "4                20      -  \n",
       "5                 8   2019  \n",
       "6                13   1991  \n",
       "7                 6   1997  \n",
       "8                30  /1995  \n",
       "9                22   1987  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML = pd.DataFrame({})\n",
    "ML['Data Name'] = Dataset_name \n",
    "ML['Data Type '] = Data_type\n",
    "ML['Task '] = Task \n",
    "ML['Attribute Type '] = Attribute_type \n",
    "ML['No of Instance '] = No_of_instances\n",
    "ML['No of Attributes '] = No_of_attributes \n",
    "ML['Year '] = Year \n",
    "ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a5d8b",
   "metadata": {},
   "source": [
    "Q9. Scrape the details of Data science recruiters Url = https://www.naukri.com/hr-recruiters-consultants\n",
    "You have to find the following details: \n",
    "A) Name\n",
    "B) Designation\n",
    "C)Company \n",
    "D)Skills they hire for \n",
    "E) Location\n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and \n",
    "click on search. All this should be done through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6deacc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.maximize_window()\n",
    "time.sleep(2)\n",
    "\n",
    "url = (\"https://www.naukri.com/hr-recruiters-consultants\")\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "68ce3d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_req=[]\n",
    "salary=[]\n",
    "discription=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dca7ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title ellipsis\"]')\n",
    "for i in title_tags:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "22ea93b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft locWdth\"]')\n",
    "for i in location_tags:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "23542de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags:\n",
    "    company=i.text\n",
    "    company_name.append(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c9474588",
   "metadata": {},
   "outputs": [],
   "source": [
    "experience_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft expwdth\"]')\n",
    "for i in experience_tags:\n",
    "    experience=i.text\n",
    "    experience_req.append(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "71428f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Salary_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft \"]')\n",
    "for i in Salary_tags:\n",
    "    Salary=i.text\n",
    "    salary.append(Salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "385b3c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Description_tags=driver.find_elements(By.XPATH,'//div[@class=\"ellipsis job-description\"]')\n",
    "for i in Description_tags:\n",
    "    Description=i.text\n",
    "    discription.append(Description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e1238d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "others=[]\n",
    "Others_tags=driver.find_elements(By.XPATH,'//ul[@class=\"tags has-description\"]')\n",
    "for i in Others_tags:\n",
    "    Others=(i.text.replace('\\n','  '))\n",
    "    others.append(Others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fdb24f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_posted=[]\n",
    "Jobpost_tags=driver.find_elements(By.XPATH,'//span[@class=\"fleft postedDate\"]')\n",
    "for i in Jobpost_tags:\n",
    "    Jobpost=i.text\n",
    "    job_posted.append(Jobpost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c0d2c801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 20 20 20 20 20 20\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title),\n",
    "      len(job_location),\n",
    "      len(company_name),\n",
    "      len(experience_req),\n",
    "      len(salary),\n",
    "      len(discription),\n",
    "      len(others),\n",
    "      len(job_posted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3ed5fb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>experience</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Description</th>\n",
       "      <th>Others</th>\n",
       "      <th>Job Posted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hiring For HR Recruiter || Fresher / EXP || Da...</td>\n",
       "      <td>Hyderabad/Secunderabad</td>\n",
       "      <td>Axis Services</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "      <td>2-3.25 Lacs PA</td>\n",
       "      <td>Day Shift female only up to 25000/- take Home ...</td>\n",
       "      <td>BPO  hr  Talent Acquisition  fresher  recruitm...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HR Recruiter</td>\n",
       "      <td>Hyderabad/ Secunderabad, Telangana(Miyapur)</td>\n",
       "      <td>Axis Clinicals Limited</td>\n",
       "      <td>0-1 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Should have good communication and presentatio...</td>\n",
       "      <td>Recruitment  Hrsd  Public speaking  Presentati...</td>\n",
       "      <td>5 Days Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Opening For Management Trainee / Executive - HR</td>\n",
       "      <td>Mumbai (All Areas)</td>\n",
       "      <td>Sahajanand Medical Technologies</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Following is the requirement : . Company - Sah...</td>\n",
       "      <td>Recruitment  Talent Acquisition  Training  MIS...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Executive/ Assistant Manager HR Generalist - P...</td>\n",
       "      <td>Pune, Maharashtra(Koregaon Park)</td>\n",
       "      <td>OASIS</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "      <td>5-6 Lacs PA</td>\n",
       "      <td>Dress Code will be Blazer, Skirt and Bun for t...</td>\n",
       "      <td>hr generalist activities  HR Information Syste...</td>\n",
       "      <td>6 Days Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HR Executive- Recruitment</td>\n",
       "      <td>Hyderabad/Secunderabad(Bharat Nagar)</td>\n",
       "      <td>Elico Healthcare Services</td>\n",
       "      <td>1-2 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Urgent Requirement for HR Executive Should hav...</td>\n",
       "      <td>Human Resource Management  Healthcare BPO  Res...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HR Executive &amp; Recruiters required @ Hyderabad...</td>\n",
       "      <td>Hyderabad/Secunderabad(Begumpet)</td>\n",
       "      <td>Tata Projects</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "      <td>1.5-5 Lacs PA</td>\n",
       "      <td>Local (Hyderabad stationed) candidates are onl...</td>\n",
       "      <td>HR Executive  Recruiter  Program  Induction  A...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jr. Executive - HR</td>\n",
       "      <td>Hyderabad/Secunderabad</td>\n",
       "      <td>Tenet Medcorp</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "      <td>1-2.5 Lacs PA</td>\n",
       "      <td>Roles and Responsibilities Desired Candidate P...</td>\n",
       "      <td>hr generalist activities  HR Operations  Hrsd ...</td>\n",
       "      <td>5 Days Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>US HR Executive</td>\n",
       "      <td>Hyderabad/Secunderabad(Madhapur)</td>\n",
       "      <td>CIS Technologies</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Shift timings: 7 pm to 4 am . Work Mode: Onsit...</td>\n",
       "      <td>onboarding  Taxation  US staffing  Staffing  H...</td>\n",
       "      <td>5 Days Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Assistant Manager - HR (Field Level Recruitment)</td>\n",
       "      <td>Bhubaneswar, Odisha, Hubli, Karnataka, Sambalp...</td>\n",
       "      <td>Muthoot Microfin</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>2.5-4 Lacs PA</td>\n",
       "      <td>. Post Graduation - MBA / PGDM in HR Full Time...</td>\n",
       "      <td>NBFC  recruitment  Mass Hiring  Bulk Hiring  L...</td>\n",
       "      <td>4 Days Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Urgent requirement For Assistant Manager HR- R...</td>\n",
       "      <td>Hyderabad/Secunderabad(Bharat Nagar)</td>\n",
       "      <td>Elico Healthcare Services</td>\n",
       "      <td>7-9 Yrs</td>\n",
       "      <td>4-7 Lacs PA</td>\n",
       "      <td>Urgent Requirement for Assistant Manager-HR. S...</td>\n",
       "      <td>Handling  Team handling  Requirements  Assista...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HR Recruiter</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Symphoni Hr</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "      <td>1-2 Lacs PA</td>\n",
       "      <td>Experience: 6 - 3 years. Job Location: Malad (...</td>\n",
       "      <td>Recruitment  Exit formalities  Talent acquisit...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hiring For HR Recruiters</td>\n",
       "      <td>Coimbatore, Tamil Nadu</td>\n",
       "      <td>Ontrack Hr Services</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>1.5-2.5 Lacs PA</td>\n",
       "      <td>Experience Graduates- -0--2 years experience i...</td>\n",
       "      <td>recruitment skills  good communication  Hiring...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HR Executive</td>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>Caliber Organisation</td>\n",
       "      <td>0-1 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Outstanding problem-solving skills. . Good tim...</td>\n",
       "      <td>HR  Business planning  Business management  Pl...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HR (Human Resources) Executive</td>\n",
       "      <td>Madurai</td>\n",
       "      <td>ASK Eva ( Ask Executive Virtual Assistance )</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Analyze the future needs of your organization ...</td>\n",
       "      <td>Medical  Resource  Management  Recruitment  Hr...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HR Executive || Nagpur || Third Party Payroll</td>\n",
       "      <td>Nagpur, Maharashtra</td>\n",
       "      <td>Tata AIA Life Insurance</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>2-2.5 Lacs PA</td>\n",
       "      <td>HIRING FOR HR RECRUITER (Male) Experience Requ...</td>\n",
       "      <td>Human Resource  Hiring  Talent Acquisition  Th...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Urgent Hiring Fresher / HR Recruiter / ONLY Fe...</td>\n",
       "      <td>Hyderabad/Secunderabad(Ameerpet)</td>\n",
       "      <td>Newzen Infotech</td>\n",
       "      <td>0-1 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Job descriptionSourcing candidates through Job...</td>\n",
       "      <td>screening  IT Recruitment  job portals  Job Po...</td>\n",
       "      <td>12 Days Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HR Manager/ Assistant manager/Executive</td>\n",
       "      <td>Nagpur, Pune, Aurangabad</td>\n",
       "      <td>Nexus Hr Consultancy</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "      <td>2.25-7.25 Lacs PA</td>\n",
       "      <td>Involved in the End-to-End recruitment Process...</td>\n",
       "      <td>Payroll Management  Joining Formalities  Exit ...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Walk in Interview - Assistant Manager-HR</td>\n",
       "      <td>Bangalore/Bengaluru(Bannerghatta +2)</td>\n",
       "      <td>Ics Realty</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "      <td>3-4 Lacs PA</td>\n",
       "      <td>Review and update policies and processes as pe...</td>\n",
       "      <td>HR Generalist Activities  HR Operations  Talen...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>HR Executive, IT Recruiter</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Bangalore/Bengaluru</td>\n",
       "      <td>Laxmi Infotech</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "      <td>2.5-6 Lacs PA</td>\n",
       "      <td>Position: HR Recruiter Role: Sourcing candidat...</td>\n",
       "      <td>hr assistant  IT Recruitment  recruitment  hr ...</td>\n",
       "      <td>7 Days Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Hr Executive And Recruiter</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Avils Automation</td>\n",
       "      <td>0-1 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Female candidates preferredEstablishing and ma...</td>\n",
       "      <td>hiring  Interviewing  Job Posting  Follow Ups ...</td>\n",
       "      <td>6 Days Ago</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  job  \\\n",
       "0   Hiring For HR Recruiter || Fresher / EXP || Da...   \n",
       "1                                        HR Recruiter   \n",
       "2     Opening For Management Trainee / Executive - HR   \n",
       "3   Executive/ Assistant Manager HR Generalist - P...   \n",
       "4                           HR Executive- Recruitment   \n",
       "5   HR Executive & Recruiters required @ Hyderabad...   \n",
       "6                                  Jr. Executive - HR   \n",
       "7                                     US HR Executive   \n",
       "8    Assistant Manager - HR (Field Level Recruitment)   \n",
       "9   Urgent requirement For Assistant Manager HR- R...   \n",
       "10                                       HR Recruiter   \n",
       "11                           Hiring For HR Recruiters   \n",
       "12                                       HR Executive   \n",
       "13                     HR (Human Resources) Executive   \n",
       "14      HR Executive || Nagpur || Third Party Payroll   \n",
       "15  Urgent Hiring Fresher / HR Recruiter / ONLY Fe...   \n",
       "16            HR Manager/ Assistant manager/Executive   \n",
       "17           Walk in Interview - Assistant Manager-HR   \n",
       "18                         HR Executive, IT Recruiter   \n",
       "19                         Hr Executive And Recruiter   \n",
       "\n",
       "                                             location  \\\n",
       "0                              Hyderabad/Secunderabad   \n",
       "1         Hyderabad/ Secunderabad, Telangana(Miyapur)   \n",
       "2                                  Mumbai (All Areas)   \n",
       "3                    Pune, Maharashtra(Koregaon Park)   \n",
       "4                Hyderabad/Secunderabad(Bharat Nagar)   \n",
       "5                    Hyderabad/Secunderabad(Begumpet)   \n",
       "6                              Hyderabad/Secunderabad   \n",
       "7                    Hyderabad/Secunderabad(Madhapur)   \n",
       "8   Bhubaneswar, Odisha, Hubli, Karnataka, Sambalp...   \n",
       "9                Hyderabad/Secunderabad(Bharat Nagar)   \n",
       "10                                             Remote   \n",
       "11                             Coimbatore, Tamil Nadu   \n",
       "12                                          Ahmedabad   \n",
       "13                                            Madurai   \n",
       "14                                Nagpur, Maharashtra   \n",
       "15                   Hyderabad/Secunderabad(Ameerpet)   \n",
       "16                           Nagpur, Pune, Aurangabad   \n",
       "17               Bangalore/Bengaluru(Bannerghatta +2)   \n",
       "18  Hyderabad/Secunderabad, Pune, Bangalore/Bengaluru   \n",
       "19                                Bangalore/Bengaluru   \n",
       "\n",
       "                                         company experience  \\\n",
       "0                                  Axis Services    0-5 Yrs   \n",
       "1                         Axis Clinicals Limited    0-1 Yrs   \n",
       "2                Sahajanand Medical Technologies    0-2 Yrs   \n",
       "3                                          OASIS    0-3 Yrs   \n",
       "4                      Elico Healthcare Services    1-2 Yrs   \n",
       "5                                  Tata Projects    1-5 Yrs   \n",
       "6                                  Tenet Medcorp    0-3 Yrs   \n",
       "7                               CIS Technologies    0-3 Yrs   \n",
       "8                               Muthoot Microfin    0-2 Yrs   \n",
       "9                      Elico Healthcare Services    7-9 Yrs   \n",
       "10                                   Symphoni Hr    0-3 Yrs   \n",
       "11                           Ontrack Hr Services    0-2 Yrs   \n",
       "12                          Caliber Organisation    0-1 Yrs   \n",
       "13  ASK Eva ( Ask Executive Virtual Assistance )    0-2 Yrs   \n",
       "14                       Tata AIA Life Insurance    0-2 Yrs   \n",
       "15                               Newzen Infotech    0-1 Yrs   \n",
       "16                          Nexus Hr Consultancy    0-5 Yrs   \n",
       "17                                    Ics Realty    0-5 Yrs   \n",
       "18                                Laxmi Infotech    0-5 Yrs   \n",
       "19                              Avils Automation    0-1 Yrs   \n",
       "\n",
       "               Salary                                        Description  \\\n",
       "0      2-3.25 Lacs PA  Day Shift female only up to 25000/- take Home ...   \n",
       "1       Not disclosed  Should have good communication and presentatio...   \n",
       "2       Not disclosed  Following is the requirement : . Company - Sah...   \n",
       "3         5-6 Lacs PA  Dress Code will be Blazer, Skirt and Bun for t...   \n",
       "4       Not disclosed  Urgent Requirement for HR Executive Should hav...   \n",
       "5       1.5-5 Lacs PA  Local (Hyderabad stationed) candidates are onl...   \n",
       "6       1-2.5 Lacs PA  Roles and Responsibilities Desired Candidate P...   \n",
       "7       Not disclosed  Shift timings: 7 pm to 4 am . Work Mode: Onsit...   \n",
       "8       2.5-4 Lacs PA  . Post Graduation - MBA / PGDM in HR Full Time...   \n",
       "9         4-7 Lacs PA  Urgent Requirement for Assistant Manager-HR. S...   \n",
       "10        1-2 Lacs PA  Experience: 6 - 3 years. Job Location: Malad (...   \n",
       "11    1.5-2.5 Lacs PA  Experience Graduates- -0--2 years experience i...   \n",
       "12      Not disclosed  Outstanding problem-solving skills. . Good tim...   \n",
       "13      Not disclosed  Analyze the future needs of your organization ...   \n",
       "14      2-2.5 Lacs PA  HIRING FOR HR RECRUITER (Male) Experience Requ...   \n",
       "15      Not disclosed  Job descriptionSourcing candidates through Job...   \n",
       "16  2.25-7.25 Lacs PA  Involved in the End-to-End recruitment Process...   \n",
       "17        3-4 Lacs PA  Review and update policies and processes as pe...   \n",
       "18      2.5-6 Lacs PA  Position: HR Recruiter Role: Sourcing candidat...   \n",
       "19      Not disclosed  Female candidates preferredEstablishing and ma...   \n",
       "\n",
       "                                               Others   Job Posted  \n",
       "0   BPO  hr  Talent Acquisition  fresher  recruitm...    1 Day Ago  \n",
       "1   Recruitment  Hrsd  Public speaking  Presentati...   5 Days Ago  \n",
       "2   Recruitment  Talent Acquisition  Training  MIS...    1 Day Ago  \n",
       "3   hr generalist activities  HR Information Syste...   6 Days Ago  \n",
       "4   Human Resource Management  Healthcare BPO  Res...    1 Day Ago  \n",
       "5   HR Executive  Recruiter  Program  Induction  A...    1 Day Ago  \n",
       "6   hr generalist activities  HR Operations  Hrsd ...   5 Days Ago  \n",
       "7   onboarding  Taxation  US staffing  Staffing  H...   5 Days Ago  \n",
       "8   NBFC  recruitment  Mass Hiring  Bulk Hiring  L...   4 Days Ago  \n",
       "9   Handling  Team handling  Requirements  Assista...    1 Day Ago  \n",
       "10  Recruitment  Exit formalities  Talent acquisit...    1 Day Ago  \n",
       "11  recruitment skills  good communication  Hiring...    1 Day Ago  \n",
       "12  HR  Business planning  Business management  Pl...    1 Day Ago  \n",
       "13  Medical  Resource  Management  Recruitment  Hr...    1 Day Ago  \n",
       "14  Human Resource  Hiring  Talent Acquisition  Th...    1 Day Ago  \n",
       "15  screening  IT Recruitment  job portals  Job Po...  12 Days Ago  \n",
       "16  Payroll Management  Joining Formalities  Exit ...    1 Day Ago  \n",
       "17  HR Generalist Activities  HR Operations  Talen...    1 Day Ago  \n",
       "18  hr assistant  IT Recruitment  recruitment  hr ...   7 Days Ago  \n",
       "19  hiring  Interviewing  Job Posting  Follow Ups ...   6 Days Ago  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'job':job_title,\n",
    "                 \"location\":job_location,\n",
    "                 \"company\":company_name,\n",
    "                 \"experience\":experience_req,\n",
    "                 \"Salary\":salary,\n",
    "                 \"Description\":discription,\n",
    "                 \"Others\":others,\n",
    "                 \"Job Posted\":job_posted})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4208b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
